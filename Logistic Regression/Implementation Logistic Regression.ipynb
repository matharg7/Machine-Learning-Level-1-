{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression</br>\n",
    "\n",
    "### This is the implementation of basic Logistic regression (One v/s All type) algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important libraries\n",
    "#as we have a one v/s all type problem, I would be using the multiprocessing module.\n",
    "#here's a link to understand how multiprocessing works with python\n",
    "# link: https://www.youtube.com/watch?v=fKl2JW_qrso\n",
    "import numpy as np\n",
    "import csv\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is write our sigmoid funciton.\n",
    "$$\n",
    "z = w^T x + b \n",
    "$$\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+exp(-z)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    l = 1 / (1 + np.exp(-Z))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute the cost function!\n",
    "\n",
    "$$\n",
    "J_w,_b(X) = \\frac{-1}{m} \\sum Ylog(A) + (1-Y)log(1-A)\n",
    "$$\n",
    "Here $A$ is value of our sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, Y, W, b):\n",
    "    m = len(X)\n",
    "    Z = np.dot(X,W) + b\n",
    "    A = sigmoid(Z)\n",
    "    A[A == 1] = 0.99999\n",
    "    A[A == 0] = 0.00001\n",
    "    loss = np.multiply( np.log(A),Y ) + np.multiply( np.log((1-A)), (1-Y) )\n",
    "    cost = -1/m * np.sum(loss)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the step to compute the gradient of our cost funciton.\n",
    "The expressions for the weights are as follows:\n",
    "\n",
    "$$\n",
    "b = b - \\frac{\\alpha}{m} \\sum (A - Y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "w = w - \\frac{\\alpha}{m} X(A-Y)^T \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_of_cost_function(X, Y, W, b):\n",
    "    m = len(X)\n",
    "    \n",
    "    Z = np.dot(X, W) + b\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    dW = 1/m * np.dot((A-Y).T, X)\n",
    "    db = 1/m * np.sum(A-Y)\n",
    "    \n",
    "    dW = dW.T\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main step! <br>\n",
    "**OPTIMIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_weights_using_gradient_descent(X, Y, W, b, learning_rate):\n",
    "    cost = compute_cost(X, Y, W, b)\n",
    "    prev_cost = 0\n",
    "    iter = 0\n",
    "    while abs(cost - prev_cost) > 0.0001:\n",
    "        iter += 1\n",
    "        dW, db = compute_gradients_of_cost_function(X,Y,W,b)\n",
    "        W = W - (learning_rate * dW)\n",
    "        b = b - (learning_rate * db)\n",
    "        prev_cost = cost\n",
    "        cost = compute_cost(X, Y, W, b)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As this is a multiclass problem, we need to keep two things in mind:\n",
    "1. We need to write a function which edits the training data for a particular class to be used for each model indipenently.\n",
    "2. We need to write a function which optimizes the model for a particular class and gives the value of W and b for that particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data_for_class(train_X, train_Y, class_label):\n",
    "    class_X = np.copy(train_X)\n",
    "    class_Y = np.copy(train_Y)\n",
    "    class_Y = np.where(class_Y == class_label, 1, 0)\n",
    "    return class_X, class_Y\n",
    "\n",
    "def optimize_class(class_label):\n",
    "    X_train, Y_train = import_data()\n",
    "    Y_train = Y_train.reshape(-1,1)\n",
    "    W = np.zeros((len(X_train[0]),1))\n",
    "    b = 0\n",
    "    class_X, class_Y = get_train_data_for_class(X_train, Y_train, class_label)\n",
    "    W_class, b_class = optimize_weights_using_gradient_descent(class_X, class_Y, W, b, 0.002)\n",
    "    W_class = W_class.reshape(1, len(W_class))\n",
    "    return W_class, b_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some functions to read and write data, lets call them the *boring functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data():\n",
    "    X = np.genfromtxt(\"train_X_lg_v2.csv\", delimiter = ',', dtype = np.float64, skip_header = 1)\n",
    "    Y = np.genfromtxt(\"train_Y_lg_v2.csv\", delimiter = ',', dtype = np.float64)  \n",
    "    return X, Y\n",
    "\n",
    "def save_model(weights, weights_file_name):\n",
    "    with open(weights_file_name, 'w') as weights_file:\n",
    "        wr = csv.writer(weights_file)\n",
    "        wr.writerows(weights)\n",
    "        weights_file.close() \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets's begin the training!!! <br>\n",
    "It needs to be noted that our multiclass classifer actually trains 4 different models, doing this one model at a time can be time consuming. We take advantage of the fact that each model is independent and hence we use the multiprocessing module to train all models simultaniously, link for the same is in the first cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X , Y = import_data()\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    class_labels = list(set(Y))\n",
    "    results = executor.map(optimize_class, class_labels)\n",
    "W_b_List = []\n",
    "for result in results:\n",
    "    W_b_List.append(result)    \n",
    "\n",
    "weights = W_b_List\n",
    "\n",
    "num_of_models = len(weights)\n",
    "W = []\n",
    "b = []\n",
    "for i in range(num_of_models):\n",
    "    W.append(weights[i][0])\n",
    "    b.append(weights[i][1])\n",
    "W = np.array(W)\n",
    "W = W.reshape(4,20)\n",
    "b = np.array(b)\n",
    "b = b.reshape(4,1)\n",
    "save_model(W, \"WEIGHTS_W_git.csv\") \n",
    "save_model(b, \"WEIGHTS_b_git.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainig completed!, Test time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another *boring function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_and_weights(test_X_file_path, weights_W_file_path, weights_b_file_path):\n",
    "    test_X = np.genfromtxt(test_X_file_path, delimiter=',', dtype=np.float64, skip_header=1)\n",
    "    W = np.genfromtxt(weights_W_file_path, delimiter=',', dtype=np.float64)\n",
    "    b = np.genfromtxt(weights_b_file_path, delimiter=',', dtype=np.float64)\n",
    "    return test_X, W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we do the prediction as following:<br>\n",
    "$$\n",
    "class = i \n",
    "$$\n",
    "<br>\n",
    "where i is class of\n",
    "$$\n",
    "max(h_1, h_2, h_3, h_4)\n",
    "$$\n",
    "$\n",
    "h_i\n",
    "$\n",
    "is the sigmoid funtion of ith class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_target_values(test_X, W, b):\n",
    "    # Write your code to Predict Target Variables\n",
    "    # HINT: You can use other functions which you've already implemented in coding assignments.\n",
    "    X = test_X\n",
    "    \n",
    "    Y = []\n",
    "    for i in range(len(b)):\n",
    "        Z = np.dot(X, W[i]) + b[i]\n",
    "        A = sigmoid(Z)\n",
    "        Y.append(A)\n",
    "    Y = np.array(Y)\n",
    "    Y = Y.T\n",
    "    Y = Y.tolist()\n",
    "    Y_pred = []\n",
    "    for i in range(len(Y)):\n",
    "        label = Y[i].index(max(Y[i]))\n",
    "        Y_pred.append(label)\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "test_X, W, b = import_data_and_weights(\"train_X_lg_v2.csv\", \"WEIGHTS_W_git.csv\", \"WEIGHTS_b_git.csv\")\n",
    "Y_pred = predict_target_values(test_X, W, b)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *I didn't have time to do the train-test split, I might do that in future and update this notebook, However this is Logistic regression multiclass classifier at the most basic level.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
