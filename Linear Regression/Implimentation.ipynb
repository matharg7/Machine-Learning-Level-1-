{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "### This is the Implementation of Linear Regression from scratch using basic Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries to be used\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Training data \n",
    "X = np.genfromtxt(\"train_X_lr.csv\", delimiter = ',', dtype = np.float64, skip_header = 1)\n",
    "Y = np.genfromtxt(\"train_Y_lr.csv\", delimiter = ',', dtype = np.float64)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfuly importing training data, we need to make it workable. Workable in the sense to use linear algebra and optimisation operations, we would restructure the data so that our mathematics works out. \n",
    "My vector form of hypothesis funciton is :\n",
    "\\begin{equation}\n",
    "h_w(X) = YPredicted = XW\n",
    "\\end{equation}\n",
    "Where W is the column vector of weights, and X is my dataset, doing matrix multiplicatioin of X and W gives a column vector of predicted values. \n",
    "However, the X needs to have one extra attribute to account for the W0 attribute, so for every training example I put x0 = 1.\n",
    "Also my Y array needs to be a column vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restructuring data\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "Y = Y.reshape((len(X),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cost Function:\n",
    "\\begin{equation}\n",
    "J(W) = {1}/{2m} * (XW - Y)^T (XW - Y)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, Y, W):\n",
    "    mse = np.average((np.square((np.dot(X,W)-Y))))\n",
    "    return mse/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's dive right into it! <br> \n",
    "Everyone already knows the basic theory of linear regression, so without wasting anymore time let's compute the **Gradient** of the *Cost Function*! <br>\n",
    "We innitialize W to be a **0** *vector* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient of cost function, \n",
    "W = np.zeros((len(X[0]),1)) \n",
    "def compute_gradient_of_cost_function(X, Y, W):\n",
    "    dW = (1/len(Y)) * (np.dot((np.dot(X,W)-Y).T, X))\n",
    "    dW = dW.T\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main part: Optimization!\n",
    "We are now going to create a function which optimizes our hypothesis funciton using the Gradient Descent algorithm.\n",
    "The algorithm is writen so that we balance the number of iterations given the value of the learning rate to reach the specified threshold(minimum). I suggest you play around with different values of learning rate to completely understand how the model works. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_weights_using_gradient_descent(X, Y, W, learning_rate):\n",
    "    cost = compute_cost(X, Y, W)\n",
    "    prev_cost = 0\n",
    "    iter = 0\n",
    "    while abs(cost - prev_cost) > 0.000001:\n",
    "        iter += 1\n",
    "        dW = compute_gradient_of_cost_function(X,Y,W)\n",
    "        W = W - (learning_rate * dW)\n",
    "        print(iter, cost)\n",
    "        prev_cost = cost\n",
    "        cost = compute_cost(X, Y, W)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Implement!\n",
    "1. Choose learning rate which reduces cost function after each iteration! I choose *0.0001* randomly!\n",
    "2. Based on the code, The optimization function would take a lot of time. However to understand the mechanics, this is the best thing to do. Feel free to change the way you would like to implement the Optimization Funciton. You could use fixed number of iterations for the same. and change the threshold values as well.<br>\n",
    "**Uncomment the below code and see how Gradient Descent goes downhill!** <br>\n",
    "**It prints: The number of Iterations v/s  Value of Cost Funciton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W = optimize_weights_using_gradient_descent(X, Y, W, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
